{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KisanMind Soil Classification Model Training\n",
    "\n",
    "**Model**: DenseNet121 (transfer learning from ImageNet)\n",
    "\n",
    "**Dataset**: Indian Region Soil Images\n",
    "\n",
    "**Classes**: Clay, Sand, Loamy Sand, Sandy Loam, Loam\n",
    "\n",
    "**Expected Training Time**: 1-2 hours on Kaggle GPU\n",
    "\n",
    "**Target Accuracy**: 90-95%\n",
    "\n",
    "---\n",
    "\n",
    "## Setup Instructions\n",
    "\n",
    "1. **Upload this notebook to Kaggle**\n",
    "2. **Upload soil images**:\n",
    "   - Create a Kaggle dataset with Sample1.jpg - Sample16.jpg\n",
    "   - Include Practical_Reading.csv\n",
    "3. **Enable GPU**: Settings â†’ Accelerator â†’ GPU T4 x2\n",
    "4. **Run all cells**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q tensorflow==2.15.0\n",
    "!pip install -q albumentations\n",
    "!pip install -q tf2onnx\n",
    "!pip install -q onnxruntime\n",
    "\n",
    "print(\"âœ… Dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import DenseNet121\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Data Preparation\n",
    "\n",
    "Organize images into train/val/test directories by soil type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read soil type labels\n",
    "# UPDATE THIS PATH to point to your uploaded dataset\n",
    "data_dir = '/kaggle/input/indian-soil-images'  # Change this!\n",
    "\n",
    "df = pd.read_csv(f'{data_dir}/Practical_Reading.csv')\n",
    "print(\"Dataset overview:\")\n",
    "print(df['Type'].value_counts())\n",
    "print(f\"\\nTotal samples: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create organized directory structure\n",
    "base_dir = '/kaggle/working/soil_data'\n",
    "os.makedirs(base_dir, exist_ok=True)\n",
    "\n",
    "for split in ['train', 'val', 'test']:\n",
    "    for soil_type in df['Type'].unique():\n",
    "        os.makedirs(f\"{base_dir}/{split}/{soil_type}\", exist_ok=True)\n",
    "\n",
    "print(\"âœ… Directory structure created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data: 70% train, 15% val, 15% test\n",
    "np.random.seed(42)\n",
    "\n",
    "for soil_type in df['Type'].unique():\n",
    "    samples = df[df['Type'] == soil_type]['Sample'].values\n",
    "    \n",
    "    # Split indices\n",
    "    train_samples, temp_samples = train_test_split(\n",
    "        samples, test_size=0.3, random_state=42\n",
    "    )\n",
    "    val_samples, test_samples = train_test_split(\n",
    "        temp_samples, test_size=0.5, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Copy images to appropriate directories\n",
    "    for sample_id in train_samples:\n",
    "        src = f\"{data_dir}/Sample{sample_id}.jpg\"\n",
    "        dst = f\"{base_dir}/train/{soil_type}/Sample{sample_id}.jpg\"\n",
    "        if os.path.exists(src):\n",
    "            shutil.copy(src, dst)\n",
    "    \n",
    "    for sample_id in val_samples:\n",
    "        src = f\"{data_dir}/Sample{sample_id}.jpg\"\n",
    "        dst = f\"{base_dir}/val/{soil_type}/Sample{sample_id}.jpg\"\n",
    "        if os.path.exists(src):\n",
    "            shutil.copy(src, dst)\n",
    "    \n",
    "    for sample_id in test_samples:\n",
    "        src = f\"{data_dir}/Sample{sample_id}.jpg\"\n",
    "        dst = f\"{base_dir}/test/{soil_type}/Sample{sample_id}.jpg\"\n",
    "        if os.path.exists(src):\n",
    "            shutil.copy(src, dst)\n",
    "    \n",
    "    print(f\"{soil_type}: {len(train_samples)} train, {len(val_samples)} val, {len(test_samples)} test\")\n",
    "\n",
    "print(\"\\nâœ… Data split complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Data Augmentation\n",
    "\n",
    "Heavy augmentation to compensate for small dataset (16 samples total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=90,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.3,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    brightness_range=[0.7, 1.3],\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Validation data (no augmentation, just rescaling)\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Load data\n",
    "img_size = (224, 224)\n",
    "batch_size = 8  # Small batch for small dataset\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    f'{base_dir}/train',\n",
    "    target_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    f'{base_dir}/val',\n",
    "    target_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_generator = val_datagen.flow_from_directory(\n",
    "    f'{base_dir}/test',\n",
    "    target_size=img_size,\n",
    "    batch_size=1,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(f\"\\nClass indices: {train_generator.class_indices}\")\n",
    "print(f\"Number of classes: {len(train_generator.class_indices)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Build Model\n",
    "\n",
    "DenseNet121 with transfer learning from ImageNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained DenseNet121\n",
    "base_model = DenseNet121(\n",
    "    weights='imagenet',\n",
    "    include_top=False,\n",
    "    input_shape=(224, 224, 3)\n",
    ")\n",
    "\n",
    "# Freeze base model initially\n",
    "base_model.trainable = False\n",
    "\n",
    "# Build complete model\n",
    "num_classes = len(train_generator.class_indices)\n",
    "\n",
    "model = models.Sequential([\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "print(\"Model architecture:\")\n",
    "model.summary()\n",
    "print(f\"\\nTotal params: {model.count_params():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Training Phase 1 - Frozen Base\n",
    "\n",
    "Train only the top layers while keeping DenseNet frozen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy', tf.keras.metrics.TopKCategoricalAccuracy(k=2, name='top_2_accuracy')]\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        'soil_model_phase1.h5',\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"\\nðŸš€ Starting Phase 1 training (frozen base)...\\n\")\n",
    "\n",
    "# Train\n",
    "history1 = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=50,  # Will early stop\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… Phase 1 training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Training Phase 2 - Fine-tuning\n",
    "\n",
    "Unfreeze the base model and fine-tune with lower learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfreeze base model\n",
    "base_model.trainable = True\n",
    "\n",
    "print(f\"Number of trainable layers: {len([l for l in model.layers if l.trainable])}\")\n",
    "\n",
    "# Recompile with lower learning rate\n",
    "model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=1e-5),  # Much lower LR\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy', tf.keras.metrics.TopKCategoricalAccuracy(k=2, name='top_2_accuracy')]\n",
    ")\n",
    "\n",
    "# Update callbacks\n",
    "callbacks_phase2 = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=15,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        'soil_model_phase2.h5',\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.2,\n",
    "        patience=7,\n",
    "        min_lr=1e-8,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"\\nðŸš€ Starting Phase 2 training (fine-tuning entire model)...\\n\")\n",
    "\n",
    "# Fine-tune\n",
    "history2 = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=100,  # Will early stop\n",
    "    callbacks=callbacks_phase2,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… Phase 2 training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Phase 1\n",
    "axes[0, 0].plot(history1.history['accuracy'], label='Train')\n",
    "axes[0, 0].plot(history1.history['val_accuracy'], label='Val')\n",
    "axes[0, 0].set_title('Phase 1: Accuracy')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Accuracy')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "axes[0, 1].plot(history1.history['loss'], label='Train')\n",
    "axes[0, 1].plot(history1.history['val_loss'], label='Val')\n",
    "axes[0, 1].set_title('Phase 1: Loss')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Loss')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "# Phase 2\n",
    "axes[1, 0].plot(history2.history['accuracy'], label='Train')\n",
    "axes[1, 0].plot(history2.history['val_accuracy'], label='Val')\n",
    "axes[1, 0].set_title('Phase 2: Accuracy (Fine-tuning)')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Accuracy')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True)\n",
    "\n",
    "axes[1, 1].plot(history2.history['loss'], label='Train')\n",
    "axes[1, 1].plot(history2.history['val_loss'], label='Val')\n",
    "axes[1, 1].set_title('Phase 2: Loss (Fine-tuning)')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Loss')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_history.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Training plots saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "print(\"\\nðŸ“Š Evaluating on test set...\\n\")\n",
    "\n",
    "test_loss, test_acc, test_top2 = model.evaluate(test_generator, verbose=1)\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Test Results:\")\n",
    "print(f\"  Test Accuracy: {test_acc*100:.2f}%\")\n",
    "print(f\"  Test Top-2 Accuracy: {test_top2*100:.2f}%\")\n",
    "print(f\"  Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "# Get predictions for confusion matrix\n",
    "test_generator.reset()\n",
    "predictions = model.predict(test_generator, verbose=1)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "true_classes = test_generator.classes\n",
    "class_labels = list(test_generator.class_indices.keys())\n",
    "\n",
    "# Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "cm = confusion_matrix(true_classes, predicted_classes)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=class_labels, \n",
    "            yticklabels=class_labels)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.savefig('confusion_matrix.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nðŸ“‹ Classification Report:\\n\")\n",
    "print(classification_report(true_classes, predicted_classes, \n",
    "                          target_names=class_labels, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final Keras model\n",
    "model.save('soil_classifier_densenet121_final.h5')\n",
    "print(\"âœ… Keras model saved: soil_classifier_densenet121_final.h5\")\n",
    "\n",
    "# Save model architecture and weights separately\n",
    "model_json = model.to_json()\n",
    "with open('soil_model_architecture.json', 'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save_weights('soil_model_weights.h5')\n",
    "\n",
    "print(\"âœ… Model architecture and weights saved separately\")\n",
    "\n",
    "# Save class mapping\n",
    "import json\n",
    "with open('soil_classes.json', 'w') as f:\n",
    "    json.dump(train_generator.class_indices, f, indent=2)\n",
    "\n",
    "print(\"âœ… Class mapping saved: soil_classes.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Convert to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tf2onnx\n",
    "\n",
    "print(\"ðŸ”„ Converting to ONNX format...\")\n",
    "\n",
    "# Convert to ONNX\n",
    "spec = (tf.TensorSpec((None, 224, 224, 3), tf.float32, name=\"input\"),)\n",
    "output_path = \"soil_classifier.onnx\"\n",
    "\n",
    "model_proto, _ = tf2onnx.convert.from_keras(\n",
    "    model,\n",
    "    input_signature=spec,\n",
    "    opset=13,\n",
    "    output_path=output_path\n",
    ")\n",
    "\n",
    "print(f\"âœ… ONNX model saved: {output_path}\")\n",
    "\n",
    "# Check file size\n",
    "import os\n",
    "onnx_size_mb = os.path.getsize(output_path) / (1024 * 1024)\n",
    "print(f\"ðŸ“¦ ONNX model size: {onnx_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Quantize to INT8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "\n",
    "print(\"ðŸ”„ Quantizing to INT8...\")\n",
    "\n",
    "quantized_model_path = \"soil_classifier_quantized.onnx\"\n",
    "\n",
    "quantize_dynamic(\n",
    "    model_input=output_path,\n",
    "    model_output=quantized_model_path,\n",
    "    weight_type=QuantType.QInt8,\n",
    "    optimize_model=True,\n",
    ")\n",
    "\n",
    "print(f\"âœ… Quantized model saved: {quantized_model_path}\")\n",
    "\n",
    "# Check quantized size\n",
    "quantized_size_mb = os.path.getsize(quantized_model_path) / (1024 * 1024)\n",
    "reduction = ((onnx_size_mb - quantized_size_mb) / onnx_size_mb) * 100\n",
    "\n",
    "print(f\"ðŸ“¦ Quantized model size: {quantized_size_mb:.2f} MB\")\n",
    "print(f\"ðŸ“‰ Size reduction: {reduction:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Validate ONNX Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "import time\n",
    "\n",
    "print(\"ðŸ§ª Validating ONNX model...\\n\")\n",
    "\n",
    "# Load ONNX model\n",
    "session = ort.InferenceSession(quantized_model_path)\n",
    "input_name = session.get_inputs()[0].name\n",
    "\n",
    "# Test on validation set\n",
    "val_generator.reset()\n",
    "onnx_predictions = []\n",
    "inference_times = []\n",
    "\n",
    "for i in range(len(val_generator)):\n",
    "    batch = val_generator[i]\n",
    "    images = batch[0]\n",
    "    \n",
    "    for img in images:\n",
    "        img_input = np.expand_dims(img, axis=0)\n",
    "        \n",
    "        # Time inference\n",
    "        start = time.time()\n",
    "        output = session.run(None, {input_name: img_input})[0]\n",
    "        inference_times.append((time.time() - start) * 1000)  # ms\n",
    "        \n",
    "        onnx_predictions.append(np.argmax(output))\n",
    "\n",
    "# Calculate metrics\n",
    "val_generator.reset()\n",
    "true_labels = val_generator.classes\n",
    "onnx_accuracy = np.mean(np.array(onnx_predictions) == true_labels)\n",
    "avg_inference_time = np.mean(inference_times)\n",
    "\n",
    "print(f\"ðŸŽ¯ ONNX Validation Results:\")\n",
    "print(f\"  Accuracy: {onnx_accuracy*100:.2f}%\")\n",
    "print(f\"  Avg Inference Time: {avg_inference_time:.2f} ms\")\n",
    "print(f\"  Min Inference Time: {np.min(inference_times):.2f} ms\")\n",
    "print(f\"  Max Inference Time: {np.max(inference_times):.2f} ms\")\n",
    "\n",
    "# Acceptance criteria\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸ† ACCEPTANCE CRITERIA CHECK:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "acc_pass = onnx_accuracy >= 0.85\n",
    "time_pass = avg_inference_time < 200\n",
    "size_pass = quantized_size_mb < 15\n",
    "\n",
    "print(f\"  Accuracy â‰¥85%: {'âœ… PASS' if acc_pass else 'âŒ FAIL'} ({onnx_accuracy*100:.1f}%)\")\n",
    "print(f\"  Inference <200ms: {'âœ… PASS' if time_pass else 'âŒ FAIL'} ({avg_inference_time:.1f}ms)\")\n",
    "print(f\"  Model size <15MB: {'âœ… PASS' if size_pass else 'âŒ FAIL'} ({quantized_size_mb:.1f}MB)\")\n",
    "\n",
    "if acc_pass and time_pass and size_pass:\n",
    "    print(\"\\nðŸŽ‰ ALL CRITERIA PASSED! Model ready for production deployment.\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸  Some criteria not met. Consider retraining with more data or adjusting hyperparameters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Create Deployment Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "print(\"ðŸ“¦ Creating deployment package...\\n\")\n",
    "\n",
    "# Create deployment zip\n",
    "with zipfile.ZipFile('kisanmind_soil_model_deployment.zip', 'w') as zipf:\n",
    "    zipf.write(quantized_model_path)\n",
    "    zipf.write('soil_classes.json')\n",
    "    zipf.write('training_history.png')\n",
    "    zipf.write('confusion_matrix.png')\n",
    "    \n",
    "    # Create README\n",
    "    readme_content = f\"\"\"# KisanMind Soil Classification Model\n",
    "\n",
    "## Model Details\n",
    "- Architecture: DenseNet121 (transfer learning)\n",
    "- Input: 224x224 RGB images\n",
    "- Output: {num_classes} soil type classes\n",
    "- Format: ONNX (INT8 quantized)\n",
    "\n",
    "## Performance Metrics\n",
    "- Test Accuracy: {test_acc*100:.2f}%\n",
    "- ONNX Accuracy: {onnx_accuracy*100:.2f}%\n",
    "- Inference Time: {avg_inference_time:.2f} ms (CPU)\n",
    "- Model Size: {quantized_size_mb:.2f} MB\n",
    "\n",
    "## Classes\n",
    "{json.dumps(train_generator.class_indices, indent=2)}\n",
    "\n",
    "## Usage\n",
    "```python\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Load model\n",
    "session = ort.InferenceSession('soil_classifier_quantized.onnx')\n",
    "\n",
    "# Preprocess image\n",
    "img = Image.open('soil.jpg').resize((224, 224))\n",
    "img_array = np.array(img).astype(np.float32) / 255.0\n",
    "img_array = np.expand_dims(img_array, axis=0)\n",
    "\n",
    "# Predict\n",
    "input_name = session.get_inputs()[0].name\n",
    "output = session.run(None, {{input_name: img_array}})[0]\n",
    "predicted_class = np.argmax(output)\n",
    "confidence = output[0][predicted_class]\n",
    "\n",
    "print(f\"Predicted: Class {{predicted_class}}, Confidence: {{confidence:.2%}}\")\n",
    "```\n",
    "\n",
    "## Training Date\n",
    "{pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "## Dataset\n",
    "Indian Region Soil Image Dataset (Kaggle)\n",
    "- Total samples: {len(df)}\n",
    "- Training: {len(train_generator.filenames)}\n",
    "- Validation: {len(val_generator.filenames)}\n",
    "- Test: {len(test_generator.filenames)}\n",
    "\"\"\"\n",
    "    \n",
    "    zipf.writestr('README.md', readme_content)\n",
    "\n",
    "print(\"âœ… Deployment package created: kisanmind_soil_model_deployment.zip\")\n",
    "print(\"\\nðŸ“¥ Download this file and deploy to KisanMind ML service!\")\n",
    "\n",
    "# Show contents\n",
    "print(\"\\nðŸ“‹ Package contents:\")\n",
    "with zipfile.ZipFile('kisanmind_soil_model_deployment.zip', 'r') as zipf:\n",
    "    for info in zipf.infolist():\n",
    "        print(f\"  - {info.filename} ({info.file_size / 1024:.1f} KB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœ… Training Complete!\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Download deployment package**: `kisanmind_soil_model_deployment.zip`\n",
    "2. **Extract files to**: `services/ml-inference/models/`\n",
    "3. **Update API code** to use the new ONNX model\n",
    "4. **Test locally** before deploying to Render\n",
    "5. **Deploy to production** on Render\n",
    "\n",
    "### Model Performance Summary:\n",
    "\n",
    "| Metric | Value | Target | Status |\n",
    "|--------|-------|--------|--------|\n",
    "| Test Accuracy | {:.1%} | â‰¥85% | {} |\n",
    "| Inference Time | {:.1f}ms | <200ms | {} |\n",
    "| Model Size | {:.1f}MB | <15MB | {} |\n",
    "\n",
    "### Training Notes:\n",
    "\n",
    "âš ï¸ **Small Dataset Warning**: This model was trained on only 16 samples. For production use:\n",
    "- Collect more real-world soil images (target: 500+ per class)\n",
    "- Use PlantDoc-style field images for better generalization\n",
    "- Implement active learning to improve with user feedback\n",
    "- Consider ensemble with other soil models\n",
    "\n",
    "---\n",
    "\n",
    "**Generated by**: KisanMind ML Training Pipeline\n",
    "\n",
    "**Training Platform**: Kaggle (Free GPU)\n",
    "\n",
    "**Model Version**: 1.0.0\".format(\n",
    "    test_acc,\n",
    "    'âœ…' if acc_pass else 'âŒ',\n",
    "    avg_inference_time,\n",
    "    'âœ…' if time_pass else 'âŒ',\n",
    "    quantized_size_mb,\n",
    "    'âœ…' if size_pass else 'âŒ'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
